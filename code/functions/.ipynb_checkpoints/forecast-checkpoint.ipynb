{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from keras.layers import Activation, Dense\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import math,sys,os\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import pandas as pd\n",
    "import importlib, os, sys\n",
    "from sklearn.metrics import mean_squared_error\n",
    "np.random.seed(5)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full data\n",
    "data = np.loadtxt('../../data/synthetic_time_series/predator_prey.txt')\n",
    "# training data\n",
    "dataset = data[0:497,1:3]\n",
    "# length of test set - 1\n",
    "test_length = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dtset, look_back=1):\n",
    "    ### Input dtset = time series\n",
    "    ### look_back = time lag of predictions\n",
    "    ### Output: two data set one the same and one lagged\n",
    "    dataX = np.zeros((np.shape(dtset)[0] - look_back - 1, np.shape(dtset)[1]))\n",
    "    dataY0 = []; dataY1 = []; dataY2 = []; dataY3 = [];\n",
    "    dataY = np.zeros((np.shape(dtset)[0] - look_back - 1, np.shape(dtset)[1]))\n",
    "    for i in range(np.shape(dtset)[0] - look_back - 1):\n",
    "        dataX[i,:] = dtset[i:(i+look_back), :]\n",
    "        dataY[i,:] = dtset[i+look_back,:]\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(X,grid):\n",
    "    error = []\n",
    "    for hyper in range(len(grid)):\n",
    "        Xtr = X[0:(np.shape(X)[0]-15),:]\n",
    "        Xtest = X[(np.shape(X)[0]-15 + 1):(np.shape(X)[0]),:]\n",
    "        pred = lstm_forecast(Xtr, np.shape(Xtest)[0]+1, grid[hyper]['neurons'],grid[hyper]['epoch'], do_cv = False)\n",
    "        error.append(np.sqrt(np.mean((Xtest - pred)**2)))\n",
    "    return(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = 32\n",
    "epoche = 300\n",
    "do_cv = False\n",
    "tstart = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_cv:\n",
    "    reg_path = {'neurons': map(int, np.linspace(8,64,5)), 'epoch': map(int, np.linspace(50,300,5))}\n",
    "    reg_path = list(ParameterGrid(reg_path))\n",
    "    training_path = training(dataset,reg_path)\n",
    "    neurons = reg_path[np.argmin(training_path)]['neurons']\n",
    "    epoche = reg_path[np.argmin(training_path)]['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_length = round(np.shape(dataset)[0] * 0.1)\n",
    "train_length = np.shape(dataset)[0] - validation_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Take the training set\n",
    "ts_training = dataset\n",
    "scaler_ts_training = preprocessing.StandardScaler().fit(ts_training)\n",
    "ts_training = preprocessing.scale(ts_training)\n",
    "num_species = ts_training.shape[1]\n",
    "ts_training_original = ts_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Reshape into X=t and Y=t+look_back\n",
    "look_back = 1\n",
    "### Here you create an array Ytrain with the column to predict scale by look_back points (e.g.,, 1)\n",
    "ts_training_tr = ts_training[0:train_length,:]\n",
    "ts_training_vl = ts_training[train_length:(train_length + validation_length),:]\n",
    "trainX, trainY = create_dataset(ts_training_tr, look_back)\n",
    "ValX, ValY = create_dataset(ts_training_vl, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = trainX.reshape((trainX.shape[0], 1, num_species))\n",
    "ValX = ValX.reshape((ValX.shape[0], 1, num_species))\n",
    "#### Take last point of the training set and start predictions from there\n",
    "ts_training_reshaped = ts_training_original.reshape((ts_training_original.shape[0], 1, num_species))\n",
    "last_point_kept = np.array(ts_training_reshaped[(np.shape(ts_training_reshaped)[0] - 1), 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(neurons, input_shape=(look_back,  num_species)))\n",
    "### Decide whether to use sparsity or not\n",
    "model.add(Dense(num_species, activation = 'linear', activity_regularizer=regularizers.l2(10e-5)))\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "sys.stdout = open(os.devnull, \"w\")\n",
    "model.fit(trainX, trainY, epochs=epoche, validation_data = (ValX, ValY), verbose=0)\n",
    "sys.stdout = sys.__stdout__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions point by point starting from the last point of the training set\n",
    "length_predictions = test_length\n",
    "realizations = 30\n",
    "next_point = np.zeros((3,num_species))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prd in range(realizations):\n",
    "    ##### Last point of the training set for predictions\n",
    "    last_point = last_point_kept\n",
    "    for i in range(0,length_predictions):\n",
    "        last_point = last_point.reshape((1, 1, num_species))\n",
    "        last_point = model.predict(last_point)\n",
    "        next_point[i,:] = next_point[i,:] + last_point\n",
    "next_point = next_point/realizations\n",
    "next_point = scaler_ts_training.inverse_transform(next_point)\n",
    "next_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
